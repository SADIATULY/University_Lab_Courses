{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Sadia Akter Tuly\n",
        "\n",
        "ID: 0432220005101030"
      ],
      "metadata": {
        "id": "0q9kgaJWd5hF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nREWSkVNN9V7",
        "outputId": "07f740fa-bc01-414e-9969-279a74b6becf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Created With 14 samples.\n",
            "\n",
            "Features (X): [[2, 30], [3, 50], [5, 40], [12, 3], [8, 80], [1, 90], [13, 14], [6, 60], [4, 65], [9, 25], [10, 10], [0, 100], [1, 50], [9, 30]]\n",
            "Labels (Y): [0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1]\n",
            "Initial Weights:\n",
            " w1=-0.2343,\n",
            " w2=0.0634,\n",
            " bias=-0.1191\n",
            " \n",
            "Epoch 100:\n",
            "Total Errors = 2\n",
            "Epoch 200:\n",
            "Total Errors = 3\n",
            "Epoch 300:\n",
            "Total Errors = 2\n",
            "Epoch 400:\n",
            "Total Errors = 2\n",
            "Epoch 500:\n",
            "Total Errors = 2\n",
            "Epoch 600:\n",
            "Total Errors = 3\n",
            "Epoch 700:\n",
            "Total Errors = 2\n",
            "Epoch 800:\n",
            "Total Errors = 3\n",
            "Epoch 900:\n",
            "Total Errors = 2\n",
            "Epoch 1000:\n",
            "Total Errors = 1\n",
            "Epoch 1100:\n",
            "Total Errors = 2\n",
            "Epoch 1200:\n",
            "Total Errors = 3\n",
            "Epoch 1300:\n",
            "Total Errors = 2\n",
            "Epoch 1400:\n",
            "Total Errors = 3\n",
            "Epoch 1500:\n",
            "Total Errors = 2\n",
            "\n",
            "Training Complete.\n",
            "Final Weights: w1=1.1807, w2=-0.0776, bias=-0.8951\n",
            "\n",
            "Test the Model\n",
            "Please enter Study Hours (or 'e' to end): 5\n",
            "Please enter Attendance Percentage: 40\n",
            "The student is likely to PASS.\n",
            "\n",
            "Test the Model\n",
            "Please enter Study Hours (or 'e' to end): 0\n",
            "Please enter Attendance Percentage: 0\n",
            "The student is likely to FAIL.\n",
            "\n",
            "Test the Model\n",
            "Please enter Study Hours (or 'e' to end): 3\n",
            "Please enter Attendance Percentage: 10\n",
            "The student is likely to PASS.\n",
            "\n",
            "Test the Model\n",
            "Please enter Study Hours (or 'e' to end): e\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# TASK 1: DATASET CREATION\n",
        "\n",
        "\"\"\"rule (Study_Hours * 10) + Attendance >= 110\n",
        "If the result is 110 or higher, the student Passes (1). Otherwise, they Fail (0).\n",
        "I have generated 14 data points to mix passing and failing scenarios.\"\"\"\n",
        "\n",
        "X = [[2, 30],[3, 50],[5, 40],[12, 3],[8, 80],[1, 90],[13, 14],[6, 60],[4, 65],[9, 25],[10, 10], [0, 100], [1, 50], [9, 30]]\n",
        "\n",
        "\n",
        "\"\"\"I used these\n",
        "\n",
        "for loops,if–else conditions,lists,functions, user input (input()),meaningful print() output\n",
        "\"\"\"\n",
        "# Automatically generating labels (Y) based on the rule\n",
        "\n",
        "Y = []\n",
        "for data in X:\n",
        "    study_score = data[0] * 10\n",
        "    attendance_score = data[1]\n",
        "    if study_score + attendance_score >= 110:\n",
        "        Y.append(1)\n",
        "    else:\n",
        "        Y.append(0)\n",
        "\n",
        "print(f\"Dataset Created With {len(X)} samples.\")\n",
        "print(\"\\nFeatures (X):\", X)\n",
        "print(\"Labels (Y):\", Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TASK 2: PERCEPTRON INITIALIZATION\n",
        "\n",
        "# Initialize weights randomly (−1 to 1) and use a small learning rate to prevent weight explosion from large input values.\n",
        "w1 = random.uniform(-0.5, 0.5)\n",
        "w2 = random.uniform(-0.5, 0.5)\n",
        "bias = random.uniform(-0.5, 0.5)\n",
        "learning_rate = 0.001\n",
        "\n",
        "print(f\"Initial Weights:\\n w1={w1:.4f},\\n w2={w2:.4f},\\n bias={bias:.4f}\\n \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TASK 3: ACTIVATION FUNCTION\n",
        "\n",
        "def step_activation(weighted_sum):\n",
        "    # Returns 1 if the weighted sum is >= 0 (Threshold), else returns 0.\n",
        "    if weighted_sum >= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TASK 4: TRAINING LOOP\n",
        "\n",
        "epochs = 1500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_error = 0\n",
        "\n",
        "    for i in range(len(X)):\n",
        "\n",
        "        # 1. Get current input features\n",
        "        x1 = X[i][0]\n",
        "        x2 = X[i][1]\n",
        "        target = Y[i]\n",
        "\n",
        "        # 2. Calculate Weighted Sum (z = x1*w1 + x2*w2 + b)\n",
        "        z = (x1 * w1) + (x2 * w2) + bias\n",
        "\n",
        "        # 3. Predict Output using Activation Function\n",
        "        prediction = step_activation(z)\n",
        "\n",
        "        # 4. Calculate Error\n",
        "        error = target - prediction\n",
        "        total_error += abs(error)\n",
        "\n",
        "        # 5. Update Weights and Bias (Perceptron Learning Rule)\n",
        "        # New_Weight = Old_Weight + (Learning_Rate * Error * Input)\n",
        "        if error != 0:\n",
        "            w1 = w1 + (learning_rate * error * x1)\n",
        "            w2 = w2 + (learning_rate * error * x2)\n",
        "            bias = bias + (learning_rate * error)\n",
        "\n",
        "\n",
        "\n",
        "    # Print loss every 100 epochs to show progress\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch+1}:\\nTotal Errors = {total_error}\")\n",
        "\n",
        "    # Stop early if converged (no errors)\n",
        "    if total_error == 0:\n",
        "        print(f\"Converged early at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nTraining Complete.\\nFinal Weights: w1={w1:.4f}, w2={w2:.4f}, bias={bias:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TASK 5: USER INPUT TESTING\n",
        "\n",
        "while True:\n",
        "    print(\"\\nTest the Model\")\n",
        "    try:\n",
        "        user_study_str = input(\"Please enter Study Hours (or 'e' to end): \")\n",
        "        if user_study_str.lower() == 'e':\n",
        "            break\n",
        "\n",
        "        user_study = float(user_study_str)\n",
        "        user_attendance = float(input(\"Please enter Attendance Percentage: \"))\n",
        "\n",
        "   # Check for invalid input: study hours or attendance cannot be less than zero\n",
        "\n",
        "        if user_study < 0 or user_attendance < 0:\n",
        "            print(\"Error: Please enter a valid positive number.\")\n",
        "            continue  # Stop current execution and restart the loop\n",
        "\n",
        "        if user_attendance > 100:\n",
        "            print(\"Error: Attendance cannot be more than 100%.\")\n",
        "            continue\n",
        "\n",
        "        # Calculate prediction only if inputs are valid\n",
        "        z_score = (user_study * w1) + (user_attendance * w2) + bias\n",
        "        result = step_activation(z_score)\n",
        "\n",
        "        if result == 1:\n",
        "            print(\"The student is likely to PASS.\")\n",
        "        else:\n",
        "            print(\"The student is likely to FAIL.\")\n",
        "\n",
        "    except ValueError:\n",
        "       # Catches invalid input like letters instead of numeric values\n",
        "        print(\"Invalid input. Please enter valid numbers only.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Short Report**\n"
      ],
      "metadata": {
        "id": "dah3RKMuQ1Db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. How the dataset was created**\n",
        "\n",
        "I made a dataset with 14 students. In this dataset, Study Hours are more important than Attendance, so Study Hours were multiplied by 10. Then I added the Study Hours score and Attendance together using a simple rule.\n",
        "\n",
        "Score=(Study_Hours×10)+Attendance\n",
        "\n",
        "If Score ≥ 110 → Pass (1)\n",
        "\n",
        "If Score < 110 → Fail (0)\n",
        "\n",
        "Because this rule forms a straight decision line, the dataset can be classified using a single perceptron.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**2.Why the learning rate was chosen**\n",
        "\n",
        "I chose a learning rate of 0.001 because the Attendance values can be large, up to 100. If I used a bigger learning rate like 0.1, the weight changes would be too big and the model could move past the correct values. A small learning rate helps the model learn slowly and safely, so it works better.\n",
        "\n",
        "\n",
        "\n",
        " **3.How they verified the model is learning**\n",
        "\n",
        "I checked whether the model was learning by watching the total_error value during training. At the beginning, the error was high. As training continued and more epochs were completed, the error slowly became smaller. When the error reached 0, it showed that the perceptron had learned properly and could classify all training data correctly."
      ],
      "metadata": {
        "id": "4gHML0qmQkes"
      }
    }
  ]
}